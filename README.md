# finetuning

*Pretraining and Fine tuning Notebook* Used PyTorch and Open AI's early Open Source Model GPT2 to develop a deeper understanding and intuition of how language models are trained. We will look at a specific simple task, Sentiment Classification, and see in two ways how we can use language models for this problem. a. Continued GPT-2 Pretraining of GPT with a Movie Plots dataset. Explore how continued pretraining affects a Language Model. This gives us a good idea how pretraining morks, and more specifically, how additional domain-specific pretraining affects the perplexity for text within this domain vs outside of the domain. b. Fine-tuning of GPT-2 for Sentiment Analysis of the IMDB dataset (using Pre/Post-Modifiers) Used both the original GPT-2 model and the model that was further pretrained on the CMU Movie Summary dataset for a Sentiment Analysis of the IMDB movie review dataset, which is part of Hugging Face datasets. We will (hopefully(!)... there are statistical fluctuations) see that fine-tuning the model that was further pre-trained on the movie plot dataset behaves somewhat better than the original gpt-2 model fine-tuned. c. IMDB Sentiment Classification with a Masked Language Model (BERT) We will also look at a Masked Language Model, specifically BERT, as a tool for Sentiment Classification of the same dataset.
